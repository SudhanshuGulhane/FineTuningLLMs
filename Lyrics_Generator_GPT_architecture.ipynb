{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3c413ee8f1114507a50f1e07926a6192": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fc04fb9092734668b1799638f8f43daf",
              "IPY_MODEL_772f68aad5cb4a2cb1e5ee76e7aa9184",
              "IPY_MODEL_427c7641d9b64cd8bd6ad2ff5f97da59"
            ],
            "layout": "IPY_MODEL_9b5a27db690a4f94a331cd969f072bad"
          }
        },
        "fc04fb9092734668b1799638f8f43daf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08f702a9ce744e77a275d56db0f9199b",
            "placeholder": "​",
            "style": "IPY_MODEL_6b52ad12f69f43cba25d7a0abe0d94d1",
            "value": "Loading dataset from disk: 100%"
          }
        },
        "772f68aad5cb4a2cb1e5ee76e7aa9184": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e330d1b0a0274816b09bc1b3a03edade",
            "max": 19,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b7d7c4cd1cb249de9cb909b40dbf8821",
            "value": 19
          }
        },
        "427c7641d9b64cd8bd6ad2ff5f97da59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d11bd835d5d4180892423eb49c4dfd2",
            "placeholder": "​",
            "style": "IPY_MODEL_48e1bdecefef40da8245f7d6eb1e8689",
            "value": " 19/19 [00:22&lt;00:00,  1.34it/s]"
          }
        },
        "9b5a27db690a4f94a331cd969f072bad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08f702a9ce744e77a275d56db0f9199b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b52ad12f69f43cba25d7a0abe0d94d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e330d1b0a0274816b09bc1b3a03edade": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7d7c4cd1cb249de9cb909b40dbf8821": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0d11bd835d5d4180892423eb49c4dfd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48e1bdecefef40da8245f7d6eb1e8689": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Installing packages and setting up google drive"
      ],
      "metadata": {
        "id": "p53f7uTN1zX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets tokenizers torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOVtXIgylbyQ",
        "outputId": "9633a5c7-a780-4141-f6f4-3d912faf5d55",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (0.21.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed datasets-3.3.2 dill-0.3.8 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wp20Jlmt62n",
        "outputId": "c94a53dc-480e-401b-c576-390cfe75291e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the dataset and pre-processing"
      ],
      "metadata": {
        "id": "8JFbnf9O1-b1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_from_disk\n",
        "ds = load_from_disk('/content/drive/My Drive/genius-song-lyrics-dataset')\n",
        "ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153,
          "referenced_widgets": [
            "3c413ee8f1114507a50f1e07926a6192",
            "fc04fb9092734668b1799638f8f43daf",
            "772f68aad5cb4a2cb1e5ee76e7aa9184",
            "427c7641d9b64cd8bd6ad2ff5f97da59",
            "9b5a27db690a4f94a331cd969f072bad",
            "08f702a9ce744e77a275d56db0f9199b",
            "6b52ad12f69f43cba25d7a0abe0d94d1",
            "e330d1b0a0274816b09bc1b3a03edade",
            "b7d7c4cd1cb249de9cb909b40dbf8821",
            "0d11bd835d5d4180892423eb49c4dfd2",
            "48e1bdecefef40da8245f7d6eb1e8689"
          ]
        },
        "id": "7VIbuaTNuHsY",
        "outputId": "4506c64b-2000-43d7-dc62-dc172b24e01c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading dataset from disk:   0%|          | 0/19 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3c413ee8f1114507a50f1e07926a6192"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['title', 'tag', 'artist', 'year', 'views', 'features', 'lyrics', 'id', 'language_cld3', 'language_ft', 'language'],\n",
              "        num_rows: 5134856\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds_english = ds['train'].filter(lambda x: x['language'] == 'en')\n",
        "ds_english"
      ],
      "metadata": {
        "id": "C84TqEDuuUJK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6062487a-3068-48cd-b8e7-580e8b12c7e6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['title', 'tag', 'artist', 'year', 'views', 'features', 'lyrics', 'id', 'language_cld3', 'language_ft', 'language'],\n",
              "    num_rows: 3374198\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_remove = ['title', 'artist', 'year', 'views', 'features', 'id', 'language_cld3', 'language_ft', 'language']\n",
        "ds_selected = ds_english.remove_columns(columns_to_remove)\n",
        "ds_selected"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSaIFujiv458",
        "outputId": "278e8ddb-2344-4a32-8a1e-d09f93519197"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['tag', 'lyrics'],\n",
              "    num_rows: 3374198\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizing the dataset"
      ],
      "metadata": {
        "id": "icV2LJgg2SyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import Dataset, load_dataset\n",
        "from tokenizers import Tokenizer, models, pre_tokenizers, trainers\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ],
      "metadata": {
        "id": "CPvM2ZdFqkQn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "genre_samples = {genre: [] for genre in ['rap', 'rb', 'rock', 'pop', 'misc', 'country']}\n",
        "sample_size = 2000\n",
        "\n",
        "for entry in ds_selected:\n",
        "    genre = entry['tag']\n",
        "    if genre in genre_samples and len(genre_samples[genre]) < sample_size:\n",
        "        genre_samples[genre].append(entry)\n",
        "    if all(len(samples) >= sample_size for samples in genre_samples.values()):\n",
        "        break\n",
        "\n",
        "# Convert to Dataset format\n",
        "ds_test = Dataset.from_list([entry for genre in genre_samples for entry in genre_samples[genre]])"
      ],
      "metadata": {
        "id": "VwjsXiFJXrXN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"lyrics_corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for lyric in ds_test[\"lyrics\"]:\n",
        "        f.write(lyric + \"\\n\")\n",
        "\n",
        "# Initialize BPE Tokenizer\n",
        "tokenizer = Tokenizer(models.BPE())\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "trainer = trainers.BpeTrainer(special_tokens=[\"<PAD>\", \"<UNK>\", \"<SOS>\", \"<EOS>\"], vocab_size=30000)\n",
        "\n",
        "tokenizer.train([\"lyrics_corpus.txt\"], trainer)\n",
        "\n",
        "tokenizer.save(\"/content/drive/My Drive/lyrics_tokenizer.json\")"
      ],
      "metadata": {
        "id": "g1hvR4zPXt-S"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load trained tokenizer\n",
        "tokenizer = Tokenizer.from_file(\"/content/drive/My Drive/lyrics_tokenizer.json\")\n",
        "\n",
        "unique_genres = sorted(set(ds_test[\"tag\"]))  # Ensure fixed ordering\n",
        "genre_to_id = {genre: idx for idx, genre in enumerate(unique_genres)}\n",
        "\n",
        "def preprocess_data(genre, lyric):\n",
        "    genre_id = genre_to_id[genre]\n",
        "    lyric_tokens = tokenizer.encode(lyric).ids\n",
        "    output_ids = torch.tensor(\n",
        "        [tokenizer.token_to_id(\"<SOS>\")] + lyric_tokens + [tokenizer.token_to_id(\"<EOS>\")], dtype=torch.long\n",
        "    )  # Target for the Decoder layer\n",
        "    return genre_id, output_ids\n",
        "\n",
        "processed_data = [preprocess_data(genre, lyric) for genre, lyric in zip(ds_test[\"tag\"], ds_test[\"lyrics\"])]\n",
        "\n",
        "input_ids = torch.tensor([x[0] for x in processed_data], dtype=torch.long)\n",
        "\n",
        "# Padding lyrics sequences for uniform batch sizes\n",
        "output_ids = pad_sequence([x[1] for x in processed_data], batch_first=True, padding_value=tokenizer.token_to_id(\"<PAD>\"))\n",
        "\n",
        "torch.save((input_ids, output_ids, genre_to_id, tokenizer), \"/content/drive/My Drive/processed_data.pt\")\n",
        "\n",
        "print(\"Preprocessing complete! Data saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hi9UjwpZhVra",
        "outputId": "96993007-d627-4a80-dcdb-a67068e2385d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing complete! Data saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids, output_ids, genre_to_id, tokenizer = torch.load(\"/content/drive/My Drive/processed_data.pt\")\n",
        "print(f\"Input shape: {input_ids.shape}, Output shape: {output_ids.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uca4o-qthn7b",
        "outputId": "064479f1-354e-4a63-ed59-42d98bbdaf43"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-f95313cb5797>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  input_ids, output_ids, genre_to_id, tokenizer = torch.load(\"/content/drive/My Drive/processed_data.pt\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([12000]), Output shape: torch.Size([12000, 24097])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the Transformer architecture step by step"
      ],
      "metadata": {
        "id": "SdRH0WEg2X_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ],
      "metadata": {
        "id": "U4epu-nOh-9I"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Positional Embeddings: for retaining the word order"
      ],
      "metadata": {
        "id": "YcIsTUeu2h9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=512):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.pe = pe.unsqueeze(0)  # Shape: (1, max_len, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1), :].to(x.device)"
      ],
      "metadata": {
        "id": "Tk9cRsdCiVRP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Building Decoder Layer which includes\n",
        "1. Masked Multi-Head Self-Attention: Prevents looking ahead in sequence\n",
        "2. Feed-Forward Network (FFN): Processes hidden representations\n",
        "3. Residual Connections & Layer Normalization : Stabilizes training"
      ],
      "metadata": {
        "id": "rhEenVg53U_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dim_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, num_heads, dropout=dropout, batch_first=True)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, dim_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(dim_ff, d_model),\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, attn_mask):\n",
        "        # Self-attention with masking\n",
        "        attn_output, _ = self.self_attn(x, x, x, attn_mask=attn_mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "\n",
        "        # Feed-forward network\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.norm2(x + self.dropout(ffn_output))\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "yOTw7a-tiqg-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Full Transformer Model includes\n",
        "1. Genre Embedding\n",
        "2. Token Embedding\n",
        "3. Positional Encoding\n",
        "4. Stacked Transformer Decoder Blocks\n",
        "5. Final Linear Projection to Vocabulary Size"
      ],
      "metadata": {
        "id": "2OsRmFQS4BBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LyricsTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, num_genres, d_model=256, num_heads=8, num_layers=6, dim_ff=512, max_len=512, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Genre embedding (maps genre to a dense vector)\n",
        "        self.genre_embedding = nn.Embedding(num_genres, d_model)\n",
        "\n",
        "        # Token embedding (maps words to vectors)\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "        # Positional encoding\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
        "\n",
        "        # Transformer decoder layers\n",
        "        self.decoder_layers = nn.ModuleList([\n",
        "            TransformerDecoderLayer(d_model, num_heads, dim_ff, dropout) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Final linear projection\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def generate_mask(self, seq_len, device):\n",
        "        \"\"\"Creates a mask to prevent future words from being seen.\"\"\"\n",
        "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).to(device)\n",
        "        return mask.masked_fill(mask == 1, float('-inf'))\n",
        "\n",
        "    def forward(self, genre, input_tokens):\n",
        "        \"\"\"\n",
        "        genre: (batch_size,) -> Genre indices\n",
        "        input_tokens: (batch_size, seq_len) -> Tokenized lyrics\n",
        "        \"\"\"\n",
        "\n",
        "        # Embed genre and expand to match sequence length\n",
        "        genre_emb = self.genre_embedding(genre).unsqueeze(1)  # Shape: (batch, 1, d_model)\n",
        "\n",
        "        # Embed input tokens\n",
        "        token_emb = self.token_embedding(input_tokens)  # (batch, seq_len, d_model)\n",
        "\n",
        "        # Apply positional encoding\n",
        "        x = self.positional_encoding(token_emb)\n",
        "\n",
        "        # Add genre embedding to the first token's position\n",
        "        x[:, 0, :] += genre_emb.squeeze(1)\n",
        "\n",
        "        # Generate causal mask\n",
        "        mask = self.generate_mask(input_tokens.shape[1], input_tokens.device)\n",
        "\n",
        "        # Pass through transformer decoder layers\n",
        "        for layer in self.decoder_layers:\n",
        "            x = layer(x, attn_mask=mask)\n",
        "\n",
        "        # Final projection to vocabulary\n",
        "        logits = self.fc_out(x)  # (batch, seq_len, vocab_size)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "NrB3CFLwiu8_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare Data for Training"
      ],
      "metadata": {
        "id": "LLetNygz4UuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class LyricsDataset(Dataset):\n",
        "    def __init__(self, input_ids, output_ids):\n",
        "        self.input_ids = input_ids\n",
        "        self.output_ids = output_ids\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.output_ids[idx]\n",
        "\n",
        "output_ids = output_ids[:, :512]  # Truncate long sequences\n",
        "\n",
        "# Initialize dataset\n",
        "dataset = LyricsDataset(input_ids, output_ids)\n",
        "\n",
        "# Define batch size\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "# Create DataLoader\n",
        "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
      ],
      "metadata": {
        "id": "3bwWWJbfjrFT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Initialize Model & Training Components"
      ],
      "metadata": {
        "id": "ZhhwGwX14eu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer again\n",
        "tokenizer = Tokenizer.from_file(\"/content/drive/My Drive/lyrics_tokenizer.json\")\n",
        "\n",
        "# Get vocabulary size\n",
        "VOCAB_SIZE = len(tokenizer.get_vocab())\n",
        "NUM_GENRES = len(genre_to_id)\n",
        "\n",
        "# Initialize model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = LyricsTransformer(vocab_size=VOCAB_SIZE, num_genres=NUM_GENRES).to(device)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.token_to_id(\"<PAD>\"))\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.98), weight_decay=1e-4)"
      ],
      "metadata": {
        "id": "MHTyQO3SjwiA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### Training Loop"
      ],
      "metadata": {
        "id": "_v1xNF4l4ioc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Training loop\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "\n",
        "    for genre, lyrics in progress_bar:\n",
        "        genre, lyrics = genre.to(device), lyrics.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(genre, lyrics[:, :-1])  # Remove last token for teacher forcing\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(logits.reshape(-1, VOCAB_SIZE), lyrics[:, 1:].reshape(-1))  # Shift target left\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Average Loss = {epoch_loss / len(train_loader)}\")\n",
        "\n",
        "    # Save model after each epoch\n",
        "    torch.save(model.state_dict(), f\"/content/drive/My Drive/lyrics_transformer_epoch{epoch+1}.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z62EeY7Fj0u5",
        "outputId": "7037164f-bdda-4981-9ea0-5e02f2b9d177"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 1500/1500 [04:07<00:00,  6.06it/s, loss=5.48]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Average Loss = 5.389508797009786\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 1500/1500 [04:07<00:00,  6.05it/s, loss=4.89]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Average Loss = 5.015917940616608\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 1500/1500 [04:07<00:00,  6.05it/s, loss=5.12]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Average Loss = 4.805589326699574\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 1500/1500 [04:08<00:00,  6.05it/s, loss=4.12]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Average Loss = 4.5994695971806845\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 1500/1500 [04:06<00:00,  6.08it/s, loss=4.4]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Average Loss = 4.376181246598562\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"/content/drive/My Drive/lyrics_transformer_epoch5.pth\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E697e85xkg4P",
        "outputId": "a214dd8e-495c-4665-b253-bf61e7f873b5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-6d40a395e405>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"/content/drive/My Drive/lyrics_transformer_epoch5.pth\", map_location='cpu'))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model evaluation\n",
        "1. Random Sampling (Selects a token based on probabilities instead of always choosing the highest one)\n",
        "2. Top-k Sampling (Restricts choices to the top k most probable tokens before sampling)"
      ],
      "metadata": {
        "id": "xB2z55PX4nee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_lyrics_sampling(model, genre_name, max_length=100, temperature=1.0):\n",
        "  model.eval()\n",
        "  genre_id = torch.tensor([genre_to_id[genre_name]], dtype=torch.long).to(device)\n",
        "  generated = [tokenizer.token_to_id(\"<SOS>\")]\n",
        "  generated_tensor = torch.tensor(generated, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "  for _ in range(max_length):\n",
        "      with torch.no_grad():\n",
        "          logits = model(genre_id, generated_tensor)[:, -1, :] / temperature  # Scaling the logits\n",
        "          probs = F.softmax(logits, dim=-1)\n",
        "          next_token = torch.multinomial(probs, num_samples=1).item()  # Sample token\n",
        "\n",
        "      if next_token == tokenizer.token_to_id(\"<EOS>\"):\n",
        "          break\n",
        "\n",
        "      generated.append(next_token)\n",
        "      generated_tensor = torch.tensor(generated, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "  return tokenizer.decode(generated)\n",
        "\n",
        "generate_lyrics_sampling(model, genre_name=\"misc\", temperature=0.7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "fVT9pvox0j2-",
        "outputId": "e7349e96-4252-45da-9753-02e47ef40982"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Don ’ t know you have a little bit of the world , but a little bit of a little bit of the world , and a little hearts , the little , and a little without a little world , and a little thing , and a little thing , and a little bit of the world . It was your little too much to think that ORS to the world . If you leave you read you , you took the little few things to make this — a little girl . It was your little thing ,'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_lyrics_top_k(model, genre_name, max_length=100, k=50, temperature=1.0):\n",
        "    model.eval()\n",
        "    genre_id = torch.tensor([genre_to_id[genre_name]], dtype=torch.long).to(device)\n",
        "    generated = [tokenizer.token_to_id(\"<SOS>\")]\n",
        "    generated_tensor = torch.tensor(generated, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        with torch.no_grad():\n",
        "            logits = model(genre_id, generated_tensor)[:, -1, :] / temperature\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            # Get top-k tokens\n",
        "            top_k_probs, top_k_indices = torch.topk(probs, k)\n",
        "            next_token = top_k_indices.squeeze(0)[torch.multinomial(top_k_probs.squeeze(0), 1)].item()\n",
        "\n",
        "        if next_token == tokenizer.token_to_id(\"<EOS>\"):\n",
        "            break\n",
        "\n",
        "        generated.append(next_token)\n",
        "        generated_tensor = torch.tensor(generated, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    return tokenizer.decode(generated)\n",
        "\n",
        "generate_lyrics_top_k(model, genre_name=\"pop\", k=60, temperature=0.9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "ozCijpL51Zzo",
        "outputId": "6dddfb6a-af81-4380-8368-adea9f2c1ff2"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"[ Verse 1 ] What ' s a thing that I ' m going to sing To take it on me When she ' s looking to my head I had to sing my heart of my heart of me And I had a heart of her heart , and me , and me I had a heart , and me and me [ Pre - Chorus ] What ' s my thoughts of me is done , my heart is like a star I like a star , and me That ' s why ' s alright ' cause\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Perplexity (PPL) Score Analysis: for evaluating lyrics generation\n",
        "- A score between 1.0 - 10 indicates -> Excellent, fluent lyrics, beyond that indicates -> decent with some uncertain words chosen\n",
        "- Further I will analyze these scores for some of the genres using the lyrics generated by the two methods defined above"
      ],
      "metadata": {
        "id": "qoP0WpcI5EUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_perplexity(model, genre_name, generated_text):\n",
        "    model.eval()\n",
        "    genre_id = torch.tensor([genre_to_id[genre_name]], dtype=torch.long).to(device)\n",
        "\n",
        "    # Tokenize generated text\n",
        "    token_ids = tokenizer.encode(generated_text).ids\n",
        "    input_tensor = torch.tensor(token_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(genre_id, input_tensor[:, :-1])  # Ignore last token for prediction\n",
        "        log_probs = F.log_softmax(logits, dim=-1)  # Convert logits to log probabilities\n",
        "\n",
        "    # Compute perplexity\n",
        "    target_tokens = input_tensor[:, 1:]  # Shift target tokens\n",
        "    token_log_probs = log_probs.gather(2, target_tokens.unsqueeze(-1)).squeeze(-1)  # Get token-wise log probs\n",
        "    avg_log_prob = token_log_probs.mean()  # Average log prob\n",
        "\n",
        "    perplexity = math.exp(-avg_log_prob.item())  # Apply exp to get PPL score\n",
        "    return perplexity"
      ],
      "metadata": {
        "id": "8xry1-Ob2Kka"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testing for Random Sampling method"
      ],
      "metadata": {
        "id": "dQZ0FlIf7FVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the score for genre `country`\n",
        "generated_lyrics1 = \"I ' m a long ride , I ' m a long way around the left this day must ' ve been on a long way I ' m a long time I ' m a long time I ' ve been a long time and I ' m a long time , long time , long time I ' m a long time on a long time I ' m a long time if you do\"\n",
        "generated_lyrics2 = \"If I could be a man ' s like a man ' s about a man who ' s talking about daddy and his kids need a man who ' s , he ' s gonna be a man who ' s talking about a man ' s son of the man ' s in New York , he ' s your man , he ' s got a man And you can ' t know , I ' d be a man ' s good man , he ' s gonna be a man\"\n",
        "print(\"Perplexity 01:\", calculate_perplexity(model, \"country\", generated_lyrics1))\n",
        "print(\"Perplexity 02:\", calculate_perplexity(model, \"country\", generated_lyrics2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKIFCWb95sLp",
        "outputId": "685cf76e-c739-40d4-c7b0-b4f0f5e8dceb"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity 01: 5.494203505034539\n",
            "Perplexity 02: 7.2454486677982075\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the score for genre `rap`\n",
        "generated_lyrics1 = \"I ' m talking , I ' m a simple , I ' m just a lot of shit , it ' s a thing , come , that ' s the world , this is what it ' s that ain ' t nobody tell me what you do ? I ' m just a feeling , this is what I ' m , my style is for you , this is what you are My people right , don ' t know what or they say I ' m talking bout to\"\n",
        "generated_lyrics2 = \"Yeah , yeah [ Verse 1 ] I get my money in the knowledge , I ' m a nasty , I ' m a hater , damn , so I ' m a different thing I ' m a hot hit the floor when I ' m a fool , I ' m a lazy shit on the hood , but I ' m a hot thing , I ' m a lot , no one , man , I ' m a hustler , you can ' t handle it , I ' m\"\n",
        "print(\"Perplexity 01:\", calculate_perplexity(model, \"rap\", generated_lyrics1))\n",
        "print(\"Perplexity 02:\", calculate_perplexity(model, \"rap\", generated_lyrics2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByfzgTdP5yWE",
        "outputId": "a9c25085-bb86-4f44-8eee-6d79c77f592d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity 01: 13.556865020324194\n",
            "Perplexity 02: 10.886218848747491\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the score for genre `rock`\n",
        "generated_lyrics1 = \"What ' s got to be this here , this old and this whole world is here , this is here with a little money Don ' t let it come , this is here but a good time Even if it comes to me ? I know about what I ' m from you Don ' t let it , this is this is here , this is here , this is here to me , this is here with you , this is here , this is right , this is here with you\"\n",
        "generated_lyrics2 = \"I ' m going to see you ' re going nowhere I ' m going to see you ' re going to see the way it ' s all that I ' ve been going to see it ' s always gonna see you ' re going to see your turn , I ' m going to see you ' re going to see you ' re going to see you ' re going to see you ' re going to see you ' re going to see you ' re going to see you\"\n",
        "print(\"Perplexity 01:\", calculate_perplexity(model, \"rock\", generated_lyrics1))\n",
        "print(\"Perplexity 02:\", calculate_perplexity(model, \"rock\", generated_lyrics2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_T_sqHHg6UC8",
        "outputId": "866a913c-5f68-4f6f-a835-3b306d413c8b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity 01: 9.79094423678175\n",
            "Perplexity 02: 3.2399278364704016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the score for genre `pop`\n",
        "generated_lyrics1 = \"I close my heart is running ' Cause they almost told me to stay , but there ' s no betray me , but I ' m a fresh , but it ' s just a lovely - yeah , but it ' s just a gh ' cause I ' m a while , now it ' s just a little bit , but it ' s just a little bit of us , but a little bit waited , but I know I ' m a record , when I ' m a couple\"\n",
        "generated_lyrics2 = \"[ Intro : Robin Thicke ] Yeah , yeah , yeah , yeah , yeah , yeah , yeah , yeah , yeah , yeah , yeah , yeah , yeah , yeah , yeah [ Verse 1 : rog Glocks ] Baby , you can ' t be no more , I ' ll be there ' Cause my time , and I ' m not everything that I ' m from a thing that I ' m saying , I GU into a show you so what I do I ' m from being honest and I\"\n",
        "print(\"Perplexity 01:\", calculate_perplexity(model, \"pop\", generated_lyrics1))\n",
        "print(\"Perplexity 02:\", calculate_perplexity(model, \"pop\", generated_lyrics2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVAuERFx6avS",
        "outputId": "2626a64a-8ca1-4bcb-b0f4-4ed8af12d056"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity 01: 13.291694520983032\n",
            "Perplexity 02: 7.987691866098484\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testing for Top-k Sampling method"
      ],
      "metadata": {
        "id": "L-h_7U0K7PH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the score for genre `country`\n",
        "generated_lyrics1 = \"The sun is going out of the end The fire is going to the world is going home [ Pre - Chorus ] The sun is going to get lost my shoes And , in my hands , we were just trying to know That the world is going to get lost my eyes And it ' s going to get lost [ Chorus ] I ' m going to get lost with my hands And all the dreams is going to get lost , you can ' t stand up high , I ' m\"\n",
        "generated_lyrics2 = \"The only one that ' s the only one that ' s just been gone , you were all gone I ' m a girl , and I ' m a girl , I ' m a girl , but you know I ' ll go , but I ' m a girl ' s what I ' m a girl , the only one that should know [ Chorus ] I ' ll be a little boy , I ' ll be a little boy , I ' ll be a little boy , it\"\n",
        "print(\"Perplexity 01:\", calculate_perplexity(model, \"country\", generated_lyrics1))\n",
        "print(\"Perplexity 02:\", calculate_perplexity(model, \"country\", generated_lyrics2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dafa5994-df72-4cff-a417-0dbd026d0df3",
        "id": "vGnJVAkO7PH6"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity 01: 10.395576277687347\n",
            "Perplexity 02: 5.852830175452453\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the score for genre `rap`\n",
        "generated_lyrics1 = \"Let us get right on , let us get here , let us get right , get right If you ever seen it out , and let ' s let us get so [ Verse 1 : Eminem ] I wanna spend this far wrong , I wanna get it ' Cause I ' m a thing , like a couple weeks of his game I ' ll make ' m just like a man , the whole world ' ll be done , that if anybody ' s happening , I ' m just\"\n",
        "generated_lyrics2 = \"You know why you ' re on the floor You know why you need a fuck you more than the ones that we been thinkin ' on the scene You know why you want every money ? You know why you want your money ? You and that ' re on the radio You know how it ' s your money ? You know you know if you want to get a fuck you life ? You know why your money ? You know why you need your money ?\"\n",
        "print(\"Perplexity 01:\", calculate_perplexity(model, \"rap\", generated_lyrics1))\n",
        "print(\"Perplexity 02:\", calculate_perplexity(model, \"rap\", generated_lyrics2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLb0qE547PH6",
        "outputId": "4e512ef8-9a9e-4259-d428-5ae1371342e2"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity 01: 21.469948331038108\n",
            "Perplexity 02: 10.276203525243108\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the score for genre `rock`\n",
        "generated_lyrics1 = \"Oh , you ' re not a little girl , your daddy ' s not the perfect , and you ' re not a little girl , but I ' m the one man that the only one Who wants to stay ? You ' re not a little girl , so I ' m the one Who gave you in the world ? Oh , I ' m the one man that I ' m the one and the man that I ' m here , but I ' m the one who ' s\"\n",
        "generated_lyrics2 = \"I see you when I see you all the world is the same You are the same old ones who are you and I see you all the world is in the world of the world I see you all the world is always over [ Chorus : Mark Hoppus ] I know you and the world was so many times before I see you and the world was so good for you [ Verse 2 : Mark Hoppus ] I had a dream of your life and I know you , all the\"\n",
        "print(\"Perplexity 01:\", calculate_perplexity(model, \"rock\", generated_lyrics1))\n",
        "print(\"Perplexity 02:\", calculate_perplexity(model, \"rock\", generated_lyrics2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BXqolz-7PH7",
        "outputId": "5a502500-88d5-4626-c13c-72037fc1b049"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity 01: 12.248845064157232\n",
            "Perplexity 02: 10.090433638679968\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the score for genre `pop`\n",
        "generated_lyrics1 = \"Yeah - I know what to get on you Yeah [ Verse 1 : Zayn ] If you should know what in the time I was always trying to be your man It ' s a lot of the world of the world I was the world , so I don ' t need you , but it ' s already too long I like it But I ' m in my favorite world for you [ Pre - Chorus ] I know , I know you ' re happy it ' s real slow I know\"\n",
        "generated_lyrics2 = \"What ' s a thing that I ' m going to sing To take it on me When she ' s looking to my head I had to sing my heart of my heart of me And I had a heart of her heart , and me , and me I had a heart , and me and me [ Pre - Chorus ] What ' s my thoughts of me is done , my heart is like a star I like a star , and me That ' s why ' s alright ' cause\"\n",
        "print(\"Perplexity 01:\", calculate_perplexity(model, \"pop\", generated_lyrics1))\n",
        "print(\"Perplexity 02:\", calculate_perplexity(model, \"pop\", generated_lyrics2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNWOJzhj7PH7",
        "outputId": "a76d7b8e-b746-45b0-f786-9f4978f19ee0"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity 01: 14.599365367187545\n",
            "Perplexity 02: 13.90482641370274\n"
          ]
        }
      ]
    }
  ]
}